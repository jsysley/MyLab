---
title: "Summary"
author: "jsysley_mac"
date: "2016年9月18日"
output: html_document
---

#Summary

##数据分析
Hmisc()包：describe()
fBasics包的basicStats()

##Linear Regression
不用包，用lm()函数，glm()函数


##聚类分析

* stats包: 包含基本的统计函数，如用于统计计算和随机数生成

    * K-均值聚类（K-Means）： kmeans(data,k,iter.max)
        * 对k选最优值，循环，目标函数聚类优度，（聚类优度）组间差/总离差 最大
        * data的类别变量要去掉
        
    * 系谱聚类（HC）: hclust(d)、cutree(tree,k)(k为类别)、rect.hclust(tree,k)(在系谱图圈出来)
        * 对k选最优值，
        * 求距离阵dist()时data的类别变量要去掉
* cluster包： 用于聚类分析，含有很多聚类相关的函数及数据集

数据集均要剔除因变量
    * K-中心点聚类（K-Medoids）：  pam(data,k)
        * data的类别变量要去掉
    
* fpc包： 含有若干聚类算法函数，如固定点聚类，线性回归聚类，DBSCAN聚类等

    * 密度聚类（DBSCAN）： dbscan(data,eps,MinPts)#半径，密度阈值
        * 要看各个样本点的距离大概在什么区间，然后循环eps的值（范围就是0-距离区间），然后内层循环密度阈值1-10
        * data的类别变量要去掉
    
* mclust包： 主要用来处理基于高斯混合模型，通过EM算法实现的聚类、分类及密度估计等问题

    * 期望最大化聚类（EM）： Mclust(data)、clustBIC()、mclust2Dplot()、densityMclust()
        * data的类别变量要去掉
        * summary(object)查看结果
    

##关联分析
Arules、arulesViz包
apriori()、eclat()
* rules0=apriori(data,parameter = list(support=0.001,confidence=0.5))#支持度，置信度

* 可视化：
arulesViz包的plot(object)
##聚类分析
Cluster、mclust、stats包

##判别分析
MASS、klaR、class、kknn包

MASS包：
    * 线性判别分析(LDA)
        * lda(formula,data,subset,na.action)
        * 预测：predict(object,data)
        * 此处data可有多余变量，包括类别
    
    * 二次判别分析(QDA)
        * qda(formula,data,subset,na.action)
        * 预测：predict(object,data)
        * 此处data可有多余变量，包括类别

klaR包：
    * 朴素贝叶斯分类
        * NaiveBayes(formula,data,subset,na.action=na.pass)
        * 预测：predict(object,data)
        * 此处data可有多余变量，包括类别
        
class包：
    * k最近邻kNN
        *knn(train,test,k=1,cl,)#k为近邻数，train,test数据集均要去掉类别，cl为train被别向量
        * 选取最优值看，循环找错误率最小的
        * 预测：predict(object,data)
        * 此处data可有多余变量，包括类别
        
kknn包：
    * 有权重的k最近邻
        * kknn(formula,train,test,na.action,=na.omit(),k=7,distance=2)#distance为闵氏距离中的参数，1为曼哈顿距离，2为欧氏距离
        * train是完整的数据包括x和y，test是仅有x的数据
        * 预测： fitted(object)


##决策树
rpart、rpart.plot，maptree,RWeka,PWake包

* CART(Classification and Regression Trees)
    
        * rpart包：
            * rpart(formula,data,method=c("anova","poisson","class","exp"),control=rpart.control(minsplit=20,minbucket=round(minsplit/3),cp=0.01,maxdepth=4)    
            * rpart(formula,data,method,minsplit，maxdepth)#y为连续型数值则用avova，离散型则用class
                * 输出结果：print(tree)
                * 输出各节点cp值：printcp(tree)
                * 详细信息：Summary(tree)
                * 预测：predict(tree,data)#此处data可带多变量
            * prune.rpart(tree,cp)、post()
        
        * rpart.plot包: rpart.plot(tree)#树的可视化
        
        * maptree包: draw.tree()
        
    * C4.5(successor of ID3)
        
        *RWeka包: J48(formula,data,subset,)

##集成学习
adabag包

* bagging(formula,data,mfinal=100)#基分类器数量 
    * 预测：predict(object,data)#data可以包含多余向量
        * 预测对象中：
            * pre.bag$confusion#测试集结果的混淆矩阵
            * pre.bag$error#测试集预测错误率

* boosting(formula,data,mfinal=100)

##随机森林
randomForest包

* importance(object,scale=TRUE)#提取随机森林模型的重要值
* MDSplot(object,factors)#产生建立随机森林模型过程中产生的临近矩阵经过标准化的坐标图
* rflmpute()#对缺失值进行插值
    * rfImpute(x,y,iter=5,ntree=300,...)
    
    * rfImpute(formula ,data,...,subset)
* treesize(x,terminal=TRUE)#查看随机森林中每一颗树的节点数，TRUE只计算最终根节点的数；FALSE则计算全部全部节点个数
* randomForest(formula,data,ntree)
* 可视化：plot()


##支持向量机
e1071包
    * svm(formula,data=NULL,subset,na.action=na.omit,scale=TRUE,kernel=c("radial","linear","polynomial","sigmoid"),type=c("C-classification","nu-classification","one-classification","eps-regression","nu-regression"")，weights=c())#type前3种针对字符型结果变量的分类方式，其中第3种方式是逻辑判别,kernel是核函数,weights为各类样本的数量比例，提高分错样本的数量可以提高准确率

可视化：
plot(object,data,formula)#formula指用来观察任意两个特征维度对模型分类的相互影响，复杂，见原文

预测：predict(object,data)#

优化：两层循环，分类方式，核函数，寻找最优，计算错误率

##神经网络
nnet包

* model1=nnet(quality~.,data = wine,subset = samp,size=4,rang=r,decay=5e-4,maxit=200)#samp为训练集行号向量，size为隐藏层中节点个数，decay为迭代精度，maxit为迭代最大次数

* model2=nnet(x,y,decay=5e-4,maxit=200,size=4,rang=r)#此时x，y要分开，且y要经过class.ind()处理

* 预测函数：
    * 第一种建模方式：predict(model1,x,type="class"),x是数据矩阵(可包含多余变量数据)，直接给出预测结果
    * 第二种建模方式：pred=predict(model2,xt)#根据模型对xt预测(不能有多余变量数据)，给出分别属于各类的概率
    
优化：对隐藏层节点数size循环寻找最优，看模型误判率，对迭代次数循环寻找最优（maxit）
    
    
##glmnet包进行LASSO变量选择
glmnet()