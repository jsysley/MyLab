---
title: "Modern Multivariate"
author: "jsysley_mac"
date: "2017年3月24日"
output: html_document
---
# 2017-03-15

* bmp包：
    * read.bmp，读入图片或矩阵
* pixmap包：
    * or = pixmaprgb(matrix)#将矩阵还原成图片
    * plot(or)
* rhl包画3D图：
    * plot3D(3维矩阵，type=n)
    * for(i in 1:10){points3D(out$scores[digit==I,1:3],col=i+1)}
* cancor(X,Y)#未知是什么
* PCA之后无score，要自己计算
    * Xi=X % * %out$xcoef


# 2017-03-22

* 在图上直接标符号
    * plot(x,y,type='n')#画空画布，假设x长度为0
    * text(x,y,1:10)#仅在相应坐标上标记符号
* 验证一个图片的图层有无差别
    * sum(abs(r[,1]-r[,3]))==0
* 读入图片
    * x=matrix(nrow=11,ncol=77660)
    * for(i in 1:11)
    {
        name=paste("D/f",i,".bmp",sep="")
        r=read.bmp(name)
        x[i,]=as.vactor(r[,,1])
    }
    m=colmeans(x)
    for(i in 1:77760)
    {
        x[,i]=scale(x[,i],center=T,scale=F)#每列减去均值
        A <- x%*%t(x)
        out=eigen(A)#求特征值
        par(mfrow=c(3,4))#画12张图
        xnew=m
        pr=pixmapRGB(matrix(xnew,nrow=243),ncol=320)
        plot(pr)
    }
    for(i in 1:11)
    {
        v=t(x)%*%out$vector[,i]/sqrt(out$values[,i])
        b=t(v)%*%x[2,]
        xnew=xnew+v%*%b
        pr=pixmapRGB(matrix(xnew,nrow=243,ncol=3201))
        plot(pr)
    }
    
##LDA
* 画图 
    * layout(matrix(c(1,2,3,3)),nrow=2,byrow=TRUE)#画布布局
    layout.show(3)#参数表示第几幅图
    library(MASS)
    wdbc=读入数据
    wdbc[wdbc==0]=0.001#后续去对数方便
    wdbc[,3:32]=log(wdbc[,3:32])
    out=lda(v2~.,wdbc[,-1])
    name(out)#查看对象的内容，还可以用attributes(out)
    plot(out)#返回的结果对象可以先plot
    layout(matrix(c(1,2,3,3),nrow=2,byrow=FALSE))
    out.predict=predict(out)
    renames(out.predict)
    LDA1=out.predict$x#判别得分
    hist(LDA1[wdbc$v2=='v2'],xlim=c(-4,6),main="",xlab='GroupB',col='blue')
    #第二幅图同理
    #画密度曲线
    plot(density(LDA1[wdbc$v2=='v2']))
    points(density(LDA1[wdbc$v2=='v2']))#加线，也可以用lines
    #混淆矩阵
    A=table(wdbc$v2,out.predict$class)
    (sum(A)-sum(diag(A)))/sum(A)#误判率


# 2017-04-05
###description
* CART:nonparametrix statiscal method.
* 再input space Rn空间中划分一定数量的矩阵(每个矩形内同属于同一类)
* stump:a single split tree with only two terminal nodes.
* 弱分类器优点：方差小
* 树分类器优点：可以处理分类的自变量

###code
```{r}
CHD <- read.table(file="F:/研究生/研一下/多元统计/DataSets/cleveland.txt",header=TRUE)
CHD <- CHD[,-15]
library(tree)
out_tree <- tree(diag~.,CHD)
plot(out_tree)
text(out_tree,pretty=1)
head(CHD$thal)
summary(out_tree)
####用另外一个包
library(rpart)
out_rpart <- rpart(diag~.,CHD)
plot(out_rpart)
text(out_rpart,type="l",extra="l")
library(raprt.plot)
rpart.plot(out_rpart,type="l",extra="l")
#换factor，用relevel函数
```

* 有序分类变量和连续变量看做同一种变量
* node impurify function.
* k classes,probability(p1,p2,...,pk)
    * 最好情况：(1,0,...,0),(0,1,...,0)...(0,0...,1)
    * 最差情况：(1/k,1/k,...,1/k)
* 熵(entropy)
* one such function f(x) if the entropy function
    * i(g) = -sum(p(k|m) x ln(p(k|m)))
    * 2分类时，i(g) = -pln(p)-(1-p)(ln(1-p))
* Gini diversity index
    * i(g) = 1-sum(p(k|m)^2)
    * 2分类时，i(g) = 2p(1-p)
    * the goodness of split,delta(s,m) = i(s)-Pl x i(ml)-Pr x i(mr)
```{r}
CHD <- cbind(CHD$age,CHD$diag)
Age <- sort(unique(CHD[,1]))
T <- length(Age)
n <- matrix(nrow=2,ncol=2)
tau_l <- numeric(T)
tau_r <- numeric(T)
dealta <- numeric(T)
for(i in 1:T)
{
    CHD_left <- matrix(CHD[CHD[,1]<=Age[i],],ncol = 2)#左节点样本
    CHD_right <- matrix(CHD[CHD[,1]>Age[i],],ncol = 2)#右节点样本
    
    n[1,1] <- sum(CHD_left[,2]==1)#左节点为1的数量
    n[1,2] <- sum(CHD_left[,2]==2)#左节点为2的数量
    n[2,1] <- sum(CHD_right[,2]==1)#右节点为1的数量
    n[2,2] <- sum(CHD_right[,2]==2)#右节点为2的数量
    n_row <- rowSums(n)#左节点的数量
    n_col <- colSums(n)#右节点的数量
    n_sum <- sum(n)#所有样本的数量
    tau_l[i] <- -n[1,1]/n_row[1]*log(n[1,1]/n_row[1])-
        n[1,2]/n_row[1]*log(n[1,2]/n_row[1])
    tau_r[i] <- -n[2,1]/n_row[2]*log(n[2,1]/n_row[2])-
        n[2,2]/n_row[2]*log(n[2,2]/n_row[2])
    dealta[i] <- 0.6899-n_row[1]/n_sum*tau_l[i]-n_row[2]/n_sum*tau_r[i]
}
par(mfrow=c(1,2))
plot(Age,tau_r,type="l",col="red",xlab="Age of split",ylab="i(tau)")
lines(Age,tau_l,type="l",col="blue")
legend(50,0.2,c("Right","Left"),lty=c(1,1),col=c("red","blue"))
```
###误判率
* r(tau) = 1- max(P(k|tau))#回带误判率（一个节点）
* 对二分类，r(tau) = 1-max(p,1-p)=min(p,1-p)
* 整棵树的误判率：R(T) = sum(k(tau)*p(tau))

###剪枝
* grow a large tree.Tmax
* Compute an estimate of R(tau) at each node,给出loss functin(penlize+regulazation)




#2017.04.26
```{r}
spam <- read.table("F:/研究生/研一下/多元统计/DataSets/spambase.txt",header=TRUE)
spam <- spam[,-58]
n <- nrow(spam)
c <- 500
gamma <- 0.002
cv <- 0
index <- sample(1:n,n,replace = FALSE)
library(e1071)
for(i in 1:10)
{
    valid_index <- index[(400*(i-1)+1):(400*i)]
    train_index <- index[-valid_index]
    train <- spam[train_index,]
    valid <- spam[valid_index,]
    out <- svm(class~.,train,kernel="radial",cost=c,gamma=gamma)
    pred <- predict(out,valid)
    cv <- cv + sum(valid$class!=pred)
}
cv/400#错误率，对应一个c-gamma
summary(out)
plot(out,train,crll~crrt)#画对应变量的分界图
#换数据集
index <- sample(1:150,150,replace=FALSE)
c <- 10
gamma <- 1:200/1000
cv2 <- numeric(200)
for(t in 1:200)
{
    for(i in 1:10)
    {
        valid_index <- index[(15*(i-1)+1):(15*i)]
        train_index <- index[-valid_index]
        train <- iris[train_index,]
        valid <- iris[valid_index,]
        out <- svm(Species~.,train,kernel="linear",cost=c,gamma=gamma[t])
        pred <- predict(out,valid)
        cv2[t] <- cv2[t] + sum(valid$Species!=pred)
    }
}
cv <- cv/150
plot(cv2~gamma,type='l')
#其中gamma200多个，cost为1个
out_tune <- tune(svm,Specied~.,data=iris,ranges=list(gamma=gamma,cost=c))
names(out_tune)
points(error~gamma,out_tune$performance,type="l",col="red")
out_tune2 <- tune(svm,Species~.,data=iris,ranges=list(gamma=seq(0,0.2,by=0.01),cost=c(1,5,10)))
out_tune2$best.parameters
plot(out_tune2)


```

