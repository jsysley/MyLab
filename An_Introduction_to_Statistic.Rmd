---
title: "An_Introduction_to_Statistic"
author: "jsysley"
date: "2016年11月3日"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#第六章 
##实验一：子集选择方法
* 最优子集选择方法
    * 使用包ISLR中的数据Hitters
```{r,eval=FALSE}
library(ISLR)
names(Hitters)
dim(Hitters)
##删除变量上存在的缺失值
Hitters <- na.omit(Hitters)
dim(Hitters)
```
* leaps包的regsubsets()函数，通过建立一系列包给定数目预测变量的最优模型，来实现最优预测变量子集的筛选，其中“最优”是使用RSS来量化。使用summary()来输出模型大小不同情况下最优的预测变量子集。默认设置下输出截至最优8变量筛选结果。nvmax参数可以设置预测变量个数。
```{r,eval=FALSE}
library(leaps)
regfit.full <- regsubsets(Salary~.,Hitters)
summary(regfit.full)#行是不同变量数下对应的选择的变量
regfit.full <- regsubsets(Salary~.,data=Hitters,nvmax = 19)
regfit.summary <- summary(regfit.full)
names(regfit.summary)
#查看Radjusted
regfit.summary$rsq
#横坐标变量个数，纵坐标对应的值画图
plot(regfit.summary$rss,xlab = "Number of Variables",ylab = "RSS",type = "l")
plot(regfit.summary$adjr2,xlab = "Number of Variables",ylab = "Adjusted RSq",type = "l")
#识别最大点
plot(regfit.summary$cp,xlab = "Number of Variables",ylab = "Cp",type = "l")
which.min(regfit.summary$cp)
points(10,regfit.summary$cp[10],col="red",cex=2,pch=20)
#使用coef()函数提取对应模型的系数
coef(regfit.full,6)#其中6不包括截距
##regsubsets()函数中的参数method="forward","backward"为向前逐步和向后逐步
```


* 使用验证集方法和交叉验证选择模型
```{r,eval=FALSE}
#定义train区分训练集和测试集
set.seed(1)
train=sample(c(TRUE,FALSE),nrow(Hitters),rep=TRUE)
test=(!train)
####使用regsubset()函数在训练集上完成模型的最优子集选择
regfit.best <- regsubsets(Salary~.,data=Hitters[train,],nvmax = 19)
#####使用测试数据生成一个回归设计矩阵
test.mat <- model.matrix(Salary~.,data = Hitters[test,])
####dii次循环从regfit.best中提取模型大小为i时最优模型的参数估计结果，并将提取的参数估计向量乘以测试集生成的回归设计矩阵，从而计算出预测值和测试集的MSE
val.errors=rep(NA,19)
for(i in 1:19)
{
    coefi=coef(regfit.best,id=i)
    pred=test.mat[,names(coefi)]%*%coefi###regsubsets()函数没有predict()函数
    val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}
val.errors
which.min(val.errors)
coef(regfit.best,10)
########编写预测函数
predict.regsubsets <- function(object,newdata,id,...)
{
    form <- as.formula(object$call[[2]])
    mat <- model.matrix(form,newdata)
    coefi <- coef(object,id=id)
    xvars=names(coefi)
    mat[,xvars]%*%coefi
}

###################交叉验证
k=10
set.seed(1)
folds <- sample(1:k,nrow(Hitters),replace=TRUE)
cv.errors <- matrix(NA,k,19,dimnames = list(NULL,paste(1:19)))
##一共有19个模型，每个模型10折
for(j in 1:k)
{
    best.fit <- regsubsets(Salary~.,data = Hitters[folds!=j,],nvmax = 19)
    for(i in 1:19)
    {
        pred <- predict(best.fit,Hitters[folds==j,],id=i)
        cv.errors[j,i]=mean((Hitters$Salary[folds==j]-pred)^2)
    }
}
############矩阵第(i,j)元素为最优j变量模型的第i折交叉验证的测试MSE，求列平均为对应模型的k交叉验证误差
mean.cv.errors <- apply(cv.errors,2,mean)
mean.cv.errors
```
##实验二：岭回归和lasso
* 用glmnet()实现岭回归和lasso
```{r,eval=FALSE}
x <- model.matrix(Salary~.,Hitters)[,-1]
y <- Hitters$Salary
```
* 岭回归(alpha=0)
```{r,eval=FALSE}
library(glmnet)
grid <- 10^seq(10,-2,length=100)
ridge.mod <- glmnet(x,y,alpha = 0,lambda = grid)
#################默认情况下，glmnet()函数对所有的变量进行了标准化，standardize=FALSE可关闭标准化
################coef()可提取存储这些系数向量的矩阵
dim(coef(ridge.mod))
```

