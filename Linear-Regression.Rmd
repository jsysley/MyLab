---
title: "Linear-Regression"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

信纸笔记

---

##Example

```{r}
#输入数据
x <- c(170,168,175,153,185,135,172)
y <- c(61,57,58,40,90,35,68)
plot(x,y)#画散点图
z <- lm(y~x+1)#表示回归，带截距项

abline(z)#画出拟合曲线，这样画的会画出训练集的范围
lines(x,fitted(z))#或者这样画也可以，仅在训练集的范围内画

summary(z)#看回归结果，主要看系数的“*”，还有R平方，以及整体模型F检验的p值
plot(z)#画各种图

model.matrix(z)#可以输出向量形式的输入正，第一列为1，接下来为各个变量

names(z)#查看模型中包括的其他数据

w <- lm(y~x-1)#表示过原点的模型
summary(w)#看回归结果
```

* residuals: 残差分析数据<br>
* coefficients: 回归方程的系数，以及推算的系数的标准差，t值，p值ֵ<br>
* F-statistic: 检验值ֵ<br>
* signif: 显著性标记<br>
    * " *** ":  极度显著<br>
    * " ** " : 高度显著<br>
    * " * "  : 显著<br>
    * " ?? "  : 不太显著?<br>
    * "   "  : 不显著<br>

```{r}
deviance(z)#计算残差平方和
residuals(z)#计算每个点的残差，生成一个向量

#生成训练集的预测值
yhat <- predict(z)
#预测新值
a <- data.frame(x=185)#用数据框类型
predict(z,a)#预测出新值
predict(z,a,interval = "prediction",level = 0.95)#预测出值，以及区间

predict(z,interval = "confidence")#对y的均值预测
predict(z,interval = "prediction")#对y的值预测

coef(z)#求出拟合模型的系数,为numeric型，可以加[1],[2]等引用个别系数

formula(z)#提取模型公式
```

---

##多元线性回归模型
* 采用数据集swiss
```{r}
str(swiss)#查看出数据
#拟合模型
swiss.lm=lm(Fertility~.,data = swiss)
summary(swiss.lm)
#变量选择
swiss.lm1=update(swiss.lm,.~.+I(Education^2))#如添Education^2
summary(swiss.lm1)

swiss.lm2=update(swiss.lm,.~.-Education)#如删除Education
summary(swiss.lm2)

```

---

##逐步回归  
* step()函数 
    * step(z,direction=" ") *
    * forward: 向前引入法 
    * backward: 向后引入法 
    * both: 逐步引入
        * step(z,direction=" ")
        * forward: ??ǰ???뷨
        * backward: ???????뷨
        * both: ????????
* 可直接回归一个包括所有变量的模型，然step()（此时默认用AIC准则选择模型），既可以完成自动的逐步回归模型，若对模型不满意，任然可以自己处理（如添加变量，删除变量等），此时用的函数： 
    * add(object,scope,...) 
    * drop(object,scope,...) 
    * 以上可以查看增加或者删除变量后AIC准则如何变化
   

```{r}
swiss.lmstep=step(swiss.lm)#查看之后自己复制公式重新拟合
summary(swiss.lmstep)

swiss.lmstepBest=lm(formula = Fertility ~ Agriculture + Education + Catholic + Infant.Mortality, data = swiss)#得出自己想要的模型
summary(swiss.lmstep)
#下面进行变量添加或者删除,
drop1(swiss.lmstep)#此时可以自己判断，判断完和之后需要自己重新用lm函数拟合出新的模型
add1(swiss.lm,~I(Education^2)+.)#添加一个Education的平方 

#用AIC准则比较多个模型
AIC(swiss.lm,swiss.lm1,swiss.lm2)

```

---

##回归诊断
###正太分布检验：shapiro.test()
```{r}
plot(residuals(swiss.lm))
shapiro.test(residuals(swiss.lm))#检验残差正太分布
```

---

###画残差图
```{r}
y.res <- resid(swiss.lm)#同residuals()
y.fit <- predict(swiss.lm)#给出拟合值
plot(y.res~y.fit)

#或者也可以：
plot(swiss.lm$residuals~swiss$Fertility)
```

---

###画标准化残差
```{r}
y.rst <- rstandard(swiss.lm)
plot(y.rst~y.fit)

```

---

###变换
```{r}
lm.new <- update(swiss.lm,sqrt(.)~.)#对y取根号
coef(lm.new)#取出新拟合模型的系数

```

---

###多重共线性的检验：kappa(z,exact=FALSE,...)
* 其中z一般用矩阵，且未相关阵，cor(data.frame),data.frame中为各种变量，样本的数据框
* exact=TRUE时表示计算精确的条件数
* k<100,多重共线性小
* 100《k《1000,多重共线性较强
* k》1000 多重共线性严重
* 找出那些多重共线性：xx=cor(data.frame);eign(xx)#看特征根

---

###广义线性模型：glm()

    fitted.model <- glm(formula,family = family.generator,data = data.frame)

做logisti回归
fm <- glm(formula,family = binomial(link=logit),data = data.frame)当不加link=logit时算出的值为概率，加了算出的值为logit值

#The Book

##Library
```{r}
#a very large collection of data sets and functions
library(MASS)

#indludes the data associated with the following
library(ISLR)
```

##Simple Linear Regression
```{r}
#use the Boston data set included in MASS library
str(Boston)#查看数据类型
fix(Boston)#更加直观的查看数据集
names(Boston)#查看数据集的各个变量名
#下面进行回归
lm.fit=lm(medv~lstat,data = Boston)#medv为因变量，lstat为自变量，简单线性回归
lm.fit#查看回归结果
summary(lm.fit)#查看详细回归结果
names(lm.fit)#查看回归之后可用的数据
coef(lm.fit)#查看变量系数
confint(lm.fit)#对变量系数进行区间估计
#prediction()可以产生confidence interval和prediction interval
predict(lm.fit,data.frame(lstat=c(5,10,15)),interval = "confidence")
predict(lm.fit,data.frame(lstat=c(5,10,15)),interval = "prediction")
#画图
plot(Boston$lstat,Boston$medv)#画出散点图
abline(lm.fit)#画出拟合直线，abline(a,b)表示画出截距为a，斜率为b的直线

par(mfrow = c(2,2))
plot(lm.fit)


#计算残差
residuals(lm.fit)#计算每个样本与拟合值的差，即残差
rstudent(lm.fit)#返回对应的标准化残差

#画出残差图
par(mfrow=c(1,1))
plot(predict(lm.fit),residuals(lm.fit))#画出残差图
plot(predict(lm.fit),rstudent(lm.fit))#画出标准化残差图

#leverage statistics
hatvalues(lm.fit)#计算每一个样本的hatvalues

plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))#返回index
```

##Multiple Linear Regression
```{r}
lm.fitm <- lm(medv~lstat+age,data = Boston)
summary(lm.fitm)

#全部变量回归
lm.fitall <- lm(medv~.,data = Boston)
summary(lm.fitall)
summary(lm.fitall)$r.sq#给出回归模型的R平方

library(car)#为了用vif()函数
vif(lm.fitall)#该数据集的vif均比较小

#下面注意到age的p值比较大，想剔除
lm.fit1 <- lm(medv~.-age,data = Boston)
summary(lm.fit1)
#或者可以用update来做
lm.fit1 <- update(lm.fit,~.-age)
summary(lm.fit1)
```

##Interaction Terms
```{r}
summary(lm(medv~lstat*age,data = Boston))#做一个有交互项的回归
```

##Non-linear Transformations of the Predictors
当用到次方的时候需要I()<br>
```{r}
lm.fit2 <- lm(medv~lstat+I(lstat^2),data = Boston)
summary(lm.fit2)
```
we use the anova() function to further quantify the extent to which the quadratic fit is superior to the linear fit<br>

```{r}
anova(lm.fit,lm.fit2)
dev.new()
plot(medv~lstat,data = Boston)
abline(lm.fit,lwd=3)
abline(lm.fit2)
#anova() perform a hypothesis test comparing the two #models.原假设是两个模型同样好，备择假设是full model表现好,这里lm.fit好
```

对于更高次的回归，建议用poly()函数
use the poly() function to create the polynomial within lm()
```{r}
lm.fit3 <- lm(medv~poly(lstat,5),data = Boston)#做5次多项式回归
summary(lm.fit3)
#对y做一个logtransformation
summary(lm(medv~log(rm),data = Boston))
```

##Qualitative Predicors
```{r}
library(ISLR)
str(Carseats)
fix(Carseats)
names(Carseats)#查看数据

lm.fit4 <- lm(Sales~.+Income:Advertising+Price:Age,data = Carseats)
summary(lm.fit4)

contrasts(Carseats$ShelveLoc)#return the coding that R uses for the dummy variables
```



